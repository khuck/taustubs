This is an example of using the TAU stub API.  The TAU stub API is useful
when a project wants to incorporate "always on" TAU measurement, but doesn't
want to complicate their build by making TAU a dependency.

To use the TAU stub API, just add tau_stub.h and tau_stub.c to your project
source code. Then add instrumentation into your application like the example
provided.

Executing the program without TAU is exactly as it was before - the overhead
of adding the measurement can be mitigated by using the C macro API instead
of the direct function calls.  That way, the API calls can be compiled out
entirely by not defining the TAU_USE_STUBS macro.

### Building the example

To build the example, type 'make'.  You'll notice that TAU isn't linked into
the program at all.  

### Running the example without TAU

To run the program, execute 

$ mpirun -n 2 ./cpi 

...or an equivalent MPI run program for your platform.  The program will
execute as normal.  The output should look like:

Process 0 on taudev
Process 1 on taudev

For 1000000 integration intervals:
  pi is approximately 3.1415926535899388, Error is 0.0000000000001457
  e is approximately 2.7182818284589505, Error is 0.0000000000000946
  sum is 1.0000000000000664, Error is 0.0000000000000664

wall clock time for pi = 0.002787
wall clock time for e  = 0.014445
wall clock time for 1  = 0.009865
Total time = 0.027096

### Running the example with TAU

To run the program with TAU, we'll use the tau_exec wrapper program, indicating
the TAU configuration we wish to use.  For example, to run with a TAU
configuration of './configure -mpi -pthread ...'. we would run:

# Add TAU to the path, if necessary - replace x86_64 with your architecture
$ PATH=$PATH:$HOME/src/tau2/x86_64/bin
$ mpirun -n 2 tau_exec -T mpi,pthread ./cpi

The tau_exec script will LD_PRELOAD the TAU libraries providing support for
TAU measurement, the MPI interposition library, and pthread API wrapping.
The output of the program should look the same, however TAU will generate two
or more profile.* files (depending on whether your MPI implementation uses
pthreads).  To see a text summary of the profile data, you can use the 'pprof'
program, in the same path as tau_exec. The profile data should look like this:

$ pprof -s -a
Reading Profile files in profile.*

FUNCTION SUMMARY (total):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call 
---------------------------------------------------------------------------------------
100.0        2,379        3,579           6           2     596546 .TAU application
 33.5          382        1,200           2 1.00001E+06     600000 main
 13.2          472          472       1E+06           0          0 f
  7.8          277          278           2           4     139005 MPI_Init() 
  1.9           64           66           2           4      33304 MPI_Finalize() 
  0.0            1            1           4           0        440 pthread_join
  0.0        0.296        0.296           4           0         74 pthread_create
  0.0        0.265        0.267           2           6        134 main-init()
  0.0        0.248        0.248           6           0         41 MPI_Reduce() 
  0.0        0.002        0.002           2           0          1 MPI_Get_processor_name() 
  0.0            0            0           2           0          0 MPI_Comm_rank() 
  0.0            0            0           2           0          0 MPI_Comm_size() 

FUNCTION SUMMARY (mean):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call 
---------------------------------------------------------------------------------------
100.0          396          596           1    0.333333     596546 .TAU application
 33.5           63          200    0.333333      166669     600000 main
 13.2           78           78      166667           0          0 f
  7.8           46           46    0.333333    0.666667     139005 MPI_Init() 
  1.9           10           11    0.333333    0.666667      33304 MPI_Finalize() 
  0.0        0.293        0.293    0.666667           0        440 pthread_join
  0.0       0.0493       0.0493    0.666667           0         74 pthread_create
  0.0       0.0442       0.0445    0.333333           1        134 main-init()
  0.0       0.0413       0.0413           1           0         41 MPI_Reduce() 
  0.0     0.000333     0.000333    0.333333           0          1 MPI_Get_processor_name() 
  0.0            0            0    0.333333           0          0 MPI_Comm_rank() 
  0.0            0            0    0.333333           0          0 MPI_Comm_size() 



